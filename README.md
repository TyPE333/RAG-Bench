Hereâ€™s your updated `README.md` that reflects the **current progress**, including:

- Completion of BM25 retriever  
- Completed evaluator with metrics and threshold checks  
- Full CLI integration  
- Reranker + Streamlit dashboard planned as Phase 2

---

```markdown
# RAG-Bench: Retrieval Evaluation Framework for RAG Pipelines

RAG-Bench is a modular evaluation framework for benchmarking the **retrieval performance** of different components in Retrieval-Augmented Generation (RAG) systems.  
It computes standard information retrieval metrics and supports plug-and-play evaluation for various retrievers and rerankers.

[ğŸ“„ Planning and Design Doc â†’](https://docs.google.com/document/d/1vuv3pliy8DV-ipau8KcpQVdp-q1hpTLvozZq0eNrvfA/edit?usp=sharing)

---

## ğŸ¯ Project Objective

- Evaluate retrievers (currently: BM25) on a fixed query set and document store.
- Compute standard retrieval metrics: Precision@K, Recall@K, and NDCG@K.
- Determine if each retriever meets predefined performance thresholds.
- Enable modular extension with rerankers, hybrid retrievers, and future QA scoring.
- Visualize retrieval behavior and failure cases through an interactive dashboard (Phase 2).

---

## ğŸ“ Project Structure

```
rag-bench/
â”œâ”€â”€ data/                     # Indexed document embeddings, metadata, and ground truth
â”œâ”€â”€ queries/                  # Query sets (JSON or CSV)
â”œâ”€â”€ retrievers/               # Retrieval implementations
â”‚   â”œâ”€â”€ base_retriever.py
â”‚   â””â”€â”€ bm25_retriever.py
â”œâ”€â”€ rerankers/                # (Planned) Reranker modules
â”‚   â”œâ”€â”€ base_reranker.py
â”‚   â””â”€â”€ bge_reranker.py       # (To be added in Phase 2)
â”œâ”€â”€ evaluation/               # Metric computation and performance checking
â”‚   â””â”€â”€ evaluator.py
â”œâ”€â”€ reports/                  # Evaluation result outputs (JSON/CSV)
â”œâ”€â”€ utils/                    # Helper utilities
â”‚   â””â”€â”€ data_loader.py
â”œâ”€â”€ dashboard/                # (Planned) Streamlit dashboard for Phase 2
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ main.py                   # CLI orchestration script
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py                  # (Optional, for pip packaging)
```

---

## ğŸ›  Core Components

| Component        | Responsibility |
|------------------|----------------|
| **Data Loader**        | Loads queries, documents, and optional ground truth labels |
| **BM25 Retriever**     | Sparse bag-of-words retriever using `rank_bm25` |
| **Evaluator**          | Computes Precision@K, Recall@K, NDCG@K per query and per retriever |
| **Threshold Checker**  | Flags whether each retriever passes/fails based on configured metrics |
| **Report Generator**   | Saves results in structured CSV/JSON formats |
| **Main Orchestrator**  | Runs the full pipeline from CLI |

---

## ğŸ“Š Current Retrieval Metrics

- **Precision@K**
- **Recall@K**
- **NDCG@K**

(Framework designed to support MRR, MAP, and LLM-based grounding checks in future phases.)

---

## ğŸš€ How To Run

```bash
python main.py \
  --retrievers bm25 \
  --query_file queries/queries.json \
  --ground_truth_file data/ground_truth.json \
  --output_dir reports/ \
  --k 5
```

âœ… CLI includes arguments for retriever name(s), top-K, input/output paths, and will support reranking in Phase 2.

---

## ğŸ§ª Phase 2 Roadmap (In Progress)

- âœ… Integrate `BGE-Reranker` (cross-encoder) to improve document ordering
- âœ… Add `base_reranker.py` to standardize reranker interface
- ğŸ”œ Update `main.py` to support reranking via CLI flag
- ğŸ”œ Build **Streamlit dashboard** for visual exploration:
  - Metric comparison across retrievers
  - Per-query inspection of retrieved docs
  - Failure mode analysis (low recall, missing ground truth, etc.)

---

## ğŸ“‹ Current Status

âœ… System design, planning, and Phase 1 MVP completed  
âœ… BM25 retriever, evaluation engine, and CLI interface implemented and tested  
ğŸš§ Phase 2: Reranker integration and dashboard development **in progress**
```

---

Would you like me to package this up as a downloadable file or just let you copy it into your repo?
