# RAG-Bench: Retrieval Evaluation Framework for RAG Pipelines

RAG-Bench is a modular evaluation framework for benchmarking the **retrieval performance** of components in Retrieval-Augmented Generation (RAG) systems.  
It computes standard IR metrics and supports plug-and-play evaluation for multiple retrievers and rerankers.

[ğŸ“„ Planning and Design Doc â†’](https://docs.google.com/document/d/1vuv3pliy8DV-ipau8KcpQVdp-q1hpTLvozZq0eNrvfA/edit?usp=sharing)

---

## ğŸ¯ Project Objective

- Evaluate retrievers (currently: BM25) on a fixed query set and document store.
- Compute standard retrieval metrics: Precision@K, Recall@K, and NDCG@K.
- Determine if each retriever or reranked pipeline meets configurable performance thresholds.
- Support modular extension to dense retrievers, hybrid methods, and LLM-grounded scoring.
- Visualize query-level behavior and retrieval quality via a Streamlit dashboard.

---

## ğŸ–¥ï¸ Dashboard Preview

![Dashboard Demo](dashboard_preview.gif)

---

## ğŸ“ Project Structure

```
rag-bench/
â”œâ”€â”€ data/                     # Corpus, metadata, and ground truth
â”œâ”€â”€ queries/                  # Query sets (JSON or CSV)
â”œâ”€â”€ retrievers/               # Retrieval modules (new retrieval scripts go here)
â”‚   â”œâ”€â”€ base_retriever.py
â”‚   â””â”€â”€ bm25_retriever.py
â”œâ”€â”€ rerankers/                # Reranking modules (new reranker scripts go here)
â”‚   â”œâ”€â”€ base_reranker.py
â”‚   â””â”€â”€ bge_reranker.py       # HuggingFace cross-encoder reranker
â”œâ”€â”€ evaluation/               # Metric computation and threshold evaluation
â”‚   â””â”€â”€ evaluator.py
â”œâ”€â”€ reports/                  # Output reports (JSON/CSV)
â”œâ”€â”€ utils/                    # Shared utilities
â”‚   â””â”€â”€ data_loader.py
â”œâ”€â”€ dashboard/                # Streamlit dashboard
â”‚   â””â”€â”€ app.py
â”œâ”€â”€ main.py                   # Orchestration script (CLI)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ›  Core Components

| Component        | Responsibility |
|------------------|----------------|
| **Retriever Interface**   | Unified API for plug-in retrievers (BM25 complete) |
| **Reranker Interface**    | Cross-encoder reranking support via `BGE-Reranker` |
| **Evaluator**             | Computes metrics and checks thresholds |
| **Report Generator**      | Outputs results to structured JSON/CSV |
| **Orchestrator (`main.py`)** | Runs retrievers/rerankers via registry + CLI |
| **Test Suite**            | Unit tests for retrievers, rerankers, and evaluation logic |

---

## ğŸ“Š Current Retrieval Metrics

- **Precision@K**
- **Recall@K**
- **NDCG@K**

Framework is extensible to:
- MRR, MAP
- LLM-based grounding and hallucination detection (future)

---

## ğŸš€ How To Run

```bash
python main.py \
  --corpus data/corpus.json \
  --queries data/queries.json \
  --gt data/qrels.json \
  --retrievers bm25 \
  --rerankers bge \
  --report reports/eval_reranked.json \
  --topk 5
```

âœ… CLI supports multiple retrievers and rerankers using a clean registry pattern.

---

## ğŸ“ˆ Phase 2 Features (Completed)

- âœ… Integrated `BGE-Reranker` (cross-encoder) for document re-scoring
- âœ… Refactored orchestration logic with `run_pipeline()` abstraction
- âœ… Unit tested reranker integration with edge cases
- âœ… Built an interactive Streamlit dashboard for:
  - ğŸ“Š Metric comparison across retrieval strategies
  - ğŸ” Per-query exploration of retrieved document content
  - âš ï¸ Failure mode filtering and performance debugging

---

## ğŸ“‹ Current Status

âœ… Phase 1 complete: retriever implementation, evaluator, CLI  
âœ… Phase 2 complete: reranker integration + Streamlit dashboard  
âœ… Project fully functional and demo-ready
